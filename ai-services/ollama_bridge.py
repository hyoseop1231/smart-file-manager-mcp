"""
Ollama integration for AI models
"""
import os
import json
import requests
import subprocess
from typing import Dict, Any, List, Optional
import base64
from pathlib import Path

class OllamaService:
    def __init__(self, base_url: str = None):
        self.base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.models = {
            "text": "qwen3:30b-a3b",  # í…ìŠ¤íŠ¸ ë¶„ì„ìš© (qwen2.5 ëŒ€ì²´)
            "vision": "llava:13b",  # ì´ë¯¸ì§€ ë¶„ì„ ì „ìš© ëª¨ë¸
            "embedding": "nomic-embed-text"  # ì„ë² ë”© ì „ìš© ëª¨ë¸
        }
        
    def check_and_pull_model(self, model_name: str) -> bool:
        """Check if model exists, pull if not"""
        try:
            # Check if model exists
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]
                
                if model_name not in model_names:
                    print(f"ğŸ“¥ ëª¨ë¸ '{model_name}'ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
                    # Pull model
                    pull_response = requests.post(
                        f"{self.base_url}/api/pull",
                        json={"name": model_name},
                        stream=True
                    )
                    
                    # Stream progress
                    for line in pull_response.iter_lines():
                        if line:
                            data = json.loads(line)
                            if "status" in data:
                                print(f"  {data['status']}", end='\r')
                    
                    print(f"\nâœ… ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                    return True
                else:
                    print(f"âœ… ëª¨ë¸ '{model_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    return True
            return False
        except Exception as e:
            print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def ensure_all_models(self):
        """Ensure all required models are available"""
        if os.getenv("SKIP_MODEL_CHECK", "false").lower() == "true":
            print("â­ï¸  ëª¨ë¸ ì²´í¬ ìŠ¤í‚µ (SKIP_MODEL_CHECK=true)")
            return
        for model_type, model_name in self.models.items():
            self.check_and_pull_model(model_name)
    
    async def generate_text(self, prompt: str, model: Optional[str] = None) -> str:
        """Generate text using Ollama"""
        if not model:
            model = self.models["text"]
            
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                }
            )
            
            if response.status_code == 200:
                return response.json().get("response", "")
            return ""
        except Exception as e:
            print(f"Error generating text: {e}")
            return ""
    
    async def analyze_image(self, image_path: str, prompt: str) -> str:
        """Analyze image using vision model (llava)"""
        try:
            # Read image and encode to base64
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            
            # Use llava vision model
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.models["vision"],
                    "prompt": prompt,
                    "images": [image_data],
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.5
                    }
                }
            )
            
            if response.status_code == 200:
                return response.json().get("response", "")
            return ""
        except Exception as e:
            print(f"Error analyzing image: {e}")
            # Fallback to filename-based classification
            file_name = Path(image_path).name
            return f"ì´ë¯¸ì§€ íŒŒì¼: {file_name}"
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate text embedding using Ollama"""
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={
                    "model": self.models["embedding"],
                    "prompt": text
                }
            )
            
            if response.status_code == 200:
                return response.json().get("embedding", [])
            return []
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return []
    
    async def categorize_file(self, file_path: str) -> Dict[str, Any]:
        """Categorize a file using AI"""
        file_type = Path(file_path).suffix.lower()
        file_name = Path(file_path).name
        
        # Image files
        if file_type in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
            prompt = f"""ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
            1. ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ë¬¸ì„œ, ìŠ¤í¬ë¦°ìƒ·, ì‚¬ì§„, ì•„íŠ¸, ë‹¤ì´ì–´ê·¸ë¨ ë“±)
            2. ì£¼ìš” ë‚´ìš© ì„¤ëª…
            3. ì¶”ì²œ í´ë”ëª… (í•œê¸€)
            
            ì‘ë‹µ í˜•ì‹:
            {{"category": "ì¹´í…Œê³ ë¦¬", "description": "ì„¤ëª…", "suggested_folder": "í´ë”ëª…"}}
            """
            
            result = await self.analyze_image(file_path, prompt)
            try:
                return json.loads(result)
            except:
                return {"category": "ì´ë¯¸ì§€", "description": file_name, "suggested_folder": "ì´ë¯¸ì§€"}
        
        # Text files
        elif file_type in ['.txt', '.md', '.py', '.js', '.java', '.cpp', '.json', '.xml']:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(1000)  # First 1000 chars
                
                prompt = f"""ë‹¤ìŒ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
                íŒŒì¼ëª…: {file_name}
                ë‚´ìš© ì¼ë¶€: {content}
                
                ì‘ë‹µ í˜•ì‹:
                {{"category": "ì¹´í…Œê³ ë¦¬", "description": "ì„¤ëª…", "suggested_folder": "í´ë”ëª…"}}
                """
                
                result = await self.generate_text(prompt)
                try:
                    return json.loads(result)
                except:
                    return {"category": "í…ìŠ¤íŠ¸", "description": file_name, "suggested_folder": "ë¬¸ì„œ"}
            except:
                return {"category": "í…ìŠ¤íŠ¸", "description": file_name, "suggested_folder": "ë¬¸ì„œ"}
        
        # Default categorization by extension
        else:
            category_map = {
                '.pdf': ("PDFë¬¸ì„œ", "PDF"),
                '.doc': ("ì›Œë“œë¬¸ì„œ", "ë¬¸ì„œ"),
                '.docx': ("ì›Œë“œë¬¸ì„œ", "ë¬¸ì„œ"),
                '.xls': ("ì—‘ì…€", "ìŠ¤í”„ë ˆë“œì‹œíŠ¸"),
                '.xlsx': ("ì—‘ì…€", "ìŠ¤í”„ë ˆë“œì‹œíŠ¸"),
                '.ppt': ("í”„ë ˆì  í…Œì´ì…˜", "í”„ë ˆì  í…Œì´ì…˜"),
                '.pptx': ("í”„ë ˆì  í…Œì´ì…˜", "í”„ë ˆì  í…Œì´ì…˜"),
                '.zip': ("ì••ì¶•íŒŒì¼", "ì••ì¶•íŒŒì¼"),
                '.mp4': ("ë™ì˜ìƒ", "ë¹„ë””ì˜¤"),
                '.mp3': ("ìŒì•…", "ì˜¤ë””ì˜¤")
            }
            
            category, folder = category_map.get(file_type, ("ê¸°íƒ€", "ê¸°íƒ€"))
            return {
                "category": category,
                "description": file_name,
                "suggested_folder": folder
            }

# Global instance
ollama_service = OllamaService()